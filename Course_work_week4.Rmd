---
title: "Peer-graded Assignment: Practical Machine Learning"
author: "Artem Braun"
date: '7th December 2017'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Prediction Assignment Writeup**  

## **Background**  

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways (Classes A, B, C, D, E).  
  
The goal of this project is to predict the manner in which participants did the exercise. This is the "classe" variable in the training set. 

## **Data set exploratory analysis**  

At first, we download and read training and testing data. 

```{r, echo=TRUE, message=FALSE, results='hide'}
Url_train_data <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
File_train_data <- "pml-training.csv"
if (!file.exists(File_train_data)) {
        download.file(Url_train_data, File_train_data, mode = "wb")
}
Url_test_data <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
File_test_data <- "pml-testing.csv"
if (!file.exists(File_test_data)) {
        download.file(Url_test_data, File_test_data, mode = "wb")
}

Train_Data <- read.csv("pml-training.csv")
Test_Data <- read.csv("pml-testing.csv")
```
  
Then we get acquainted with training data by looking at the dataframe structure, target variable (classe) and NA observations.

```{r, echo=TRUE, message=FALSE, results='hide'}
dim(Train_Data)
str(Train_Data[,1:10])
```

```{r, echo=TRUE, message=FALSE, results='asis'}
classe <- as.data.frame(table(Train_Data$classe))
knitr::kable(classe, format = "html")
```

```{r, echo=TRUE, message=FALSE, results='asis'}
# NA % count
mean(is.na(Train_Data))
```
  
Looking at the data we can conclude that the first 7 variables are not predictors in terms of relationship with 'classe', so we can delete them. Additionally, there are more than 40% of NAs in dataframe, which should be eliminated. We use 'Near Zero Variance' method as well. 

```{r, echo=TRUE, message=FALSE, results='asis'}
Train_Data <- Train_Data[,-c(1:7)]

#deleting variables with near zero variance
library(caret)
NZV <- nearZeroVar(Train_Data,saveMetrics=TRUE)
Train_Data <- Train_Data[, (NZV$nzv==FALSE)]

#deleting columns with NAs
Count_NA <- sapply(Train_Data, function(x) sum(length(which(is.na(x)))))
Count_NA_table <- as.data.frame(table(Count_NA))
knitr::kable(Count_NA_table, format = "html")
```
  
As we can see, there are either 0 or 19216 NAs in columns. We use this finding for eliminating unnecessary columns.

```{r, echo=TRUE, message=FALSE, results='asis'}
Train_Data <- Train_Data[, (Count_NA < 19216)]
dim(Train_Data)
```
  
53 variables is something we can work with. 

## **Building the models**  

At first, we split our original training data into training and test sets (70/30).

```{r, echo=TRUE, message=FALSE, results='hide'}
set.seed(12062017)
training <- createDataPartition(Train_Data$classe, p=0.7, list=FALSE)
Train_split <- Train_Data[training,]
Test_split <- Train_Data[-training,]
```
  
We are going to build two of the most used and accurate model types: Random Forest and Gradient Boosting with trees.
Models will be built on the training subset and then evaluated on the test subset.
Additionally, we chose K-Fold cross validation (5-Fold to save the time), which involves splitting the dataset into 5 subsets. Larger k means less bias and more variance.  
In order to control the process we use 'verboseIter'.  

```{r, echo=TRUE, message=FALSE, results='hide', cache=TRUE}
trainctrl <- trainControl(method = "cv", 5, verboseIter = TRUE)
RF_fitted <- train(classe ~ ., data=Train_split, method="rf", trControl = trainctrl)
GBM_fitted <- train(classe ~ ., data=Train_split, method="gbm", trControl = trainctrl)
```

Now we test our models on the testing subset of original training data and calculate accuracy.

```{r, echo=TRUE, message=FALSE, results='asis'}
RF_test_on_plit <- predict(RF_fitted, Test_split)
confusionMatrix(RF_test_on_plit, Test_split$classe)$overall[1]

GBP_test_on_split <- predict(GBM_fitted, Test_split)
confusionMatrix(GBP_test_on_split, Test_split$classe)$overall[1]
```

Out-of-sample error = 1 - Accuracy (%).
Expected out-of-sample error for Random Forest model is 0.8%. 
Expected out-of-sample error for Gradient Boosting model is 4%, which is dramatically higher. 

## **Conclusion**  
  
Random Forest model provided better accuracy (99.2%) than Gradient Boosting model (96%). 
Therefore, we will use Random Forest model for predictions on original test data. 